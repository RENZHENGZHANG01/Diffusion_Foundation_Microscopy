# Microscopy Diffusion Training Configuration
# DiT-XL/8 based 512x512 generation

# Data configuration
data:
  foundation_path: "/mnt/e/Microscopy_dataset/processed/foundation"
  degraded_path: "/mnt/e/Microscopy_dataset/processed/degraded"

# Model configuration
model:
  architecture: "DiT-XL/8"        # DiT-XL with patch size 8 for 512x512
  input_size: [512, 512]          # Direct pixel-level input size
  patch_size: 8                   # Patch size for DiT (512//8 = 64 patches per dim)
  hidden_size: 1152               # DiT-XL hidden dimension
  # Settings used when training in VAE latent space
  latent_size: 64                 # Spatial size of VAE latents (e.g., 64x64)
  latent_channels: 4              # Channel count of VAE latents (SD VAE uses 4)
  condition_size: [512, 512]
  target_size: [512, 512]

# VAE configuration
vae:
  enabled: false                      # Set to true to train in latent space
  model_id: "stabilityai/sd-vae-ft-ema"  # VAE model (only used if enabled: true)

# Training configuration
train:
  accumulate_grad_batches: 1            # increase for gradient accumulation if memory-bound
  dataloader_workers: 8                 # tune per machine; A100/H100 can handle higher
  save_interval: 1000
  gradient_checkpointing: false   # DiT doesn't need this
  save_path: "microscopy_runs"

  # Logging (optional)
  wandb:
    project: "MicroscopyDiffusion"

# Optimizer configuration
optimizer:
  lr: 1e-4
  weight_decay: 0.01
  min_lr: 1e-6

# Scheduler configuration
scheduler:
  type: "EDMEulerScheduler"           # EDM scheduler for better sampling
  num_train_timesteps: 1000
  sigma_min: 0.002
  sigma_max: 80.0
  sigma_data: 0.5
  rho: 7.0
  prediction_type: "v_prediction"    # v-parameterization for stability

# EMA (Exponential Moving Average) configuration
ema:
  enabled: true
  decay: 0.9999
  update_every: 1
  start_step: 2000

# Real-time monitoring
monitoring:
  log_every_n_steps: 10
  val_check_interval: 100
  progress_bar: true
  save_top_k: 4
  monitor_metric: "val_loss"
  # Image logging
  image_log_every_n_steps: 200
  image_log_max_images: 8
  
  # Real-time plots
  plot_loss: true
  plot_lr: true
  plot_samples: true
  sample_every_n_epochs: 5

## Phase definitions (production scale)
phases:
  # Phase 1: Unconditional prior on ~100k images
  - name: "phase1_unconditional"
    type: "unconditional"
    epochs: 10                          # or tune based on dataset size
    batch_size: 16                      # adjust per GPU memory
    max_steps: 6250                     # ~100k images at batch_size=16
    datasets: ["sr_caco2", "fmd", "neuronal_cells", "rxrx1"]
    epochs_to_save: [3, 6, 9, 10]
    description: "Unconditional DiT-XL/8 prior on microscopy foundation (~100k images)"

  # Phase 2: Conditional control on 60–200k images
  - name: "phase2_conditional"
    type: "conditional"
    epochs: 12                          # or tune based on dataset size
    batch_size: 16
    max_steps: 7500                     # ~120k images; use 3750 for ~60k, 12500 for ~200k
    datasets: ["sr_caco2", "fmd"]
    base_from_phase: "phase1_unconditional"
    condition_types: ["super_resolution", "denoising", "deblurring"]
    condition_dropout: 0.1
    epochs_to_save: [4, 8, 12]
    description: "Conditional DiT with manual + real degradations (60–200k images)"

    # Manual degradation augmentation
    use_manual_degradation: true
    manual_degradation_ratio: 0.6

    # Mapping degradation modes to tasks
    degradation_to_task_map:
      blur: "deblurring"
      noise: "denoising"
      downsample: "super_resolution"

    degradation_modes: ["blur", "noise", "downsample"]
    degradation_params:
      blur:
        psf_scale: [1.0, 3.0]
        extra_sigma_px: [0.0, 2.0]
      noise:
        target_snr_at_full_scale: [10, 50]
        read_noise_e: [1.0, 3.0]
      downsample:
        factor: [2, 6]
        upsample_back: true